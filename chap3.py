#! /usr/bin/env python3

r"""
Notes from Chapter 3 of Hans Petter Langtangen's book "Finite Difference
Computing with Exponential Decay Models".

Problem formulation: Using finite difference methods find u(t) such that:
           u'(t) = -a(t) u(t) + b(t)     t âˆˆ (0, T]     u(0) = I       (1)
               u'(t) = f(u, t)     t âˆˆ (0, T]     u(0) = I             (2)

"""

from copy import deepcopy
from functools import partial
from typing import List

import numpy as np
import sympy as sym
import scipy.linalg
import scipy.optimize
import matplotlib.gridspec as gridspec
from matplotlib import pyplot as plt

from utils.solver import compute_rates
from utils.solver import solver_chap3 as solver


__all__ = ['generalisation', 'verification', 'convergence', 'systems',
           'generic_FO_ODE', 'bdf2', 'leapfrog', 'rk2', 'taylor_series',
           'adams_bashforth', 'rk4', 'compare_schemes']

IMGDIR = './img/chap3/'
"""Path to store images."""
STR_FMT = '{0}\n{1}\n'
"""String formatting for printing to standard output."""


def generalisation() -> None:
    """
    Generalisation: including a variable coefficient and a source term.

    Notes
    ----------
    We now start to look at the generalisations u' = -a(t) u and
    u' = -a(t) u + b(t). Verification can no longer make use of an exact
    solution of the numerical problem, so we make use of manufactured
    solutions, for deriving an exact solution of the ODE problem, and then we
    can compute empirical convergence rates for the method and see if these
    coincide with the expected rates from theory.

    We start by considered the case where a depends on time

               u'(t) = -a(t) u(t)       t âˆˆ (0, T]     u(0) = I       (2)

    A Forward Euler scheme consists of evaluating (2) at t = tn, and
    approximating with a forward difference [Dâ‚œâº u]â¿.

                        (uâ¿âºÂ¹ - uâ¿) / Î”t = -a(tn) uâ¿

    The Backward scheme becomes

                        (uâ¿ - uâ¿â»Â¹) / Î”t = -a(tn) uâ¿

    The Crank-Nicolson scheme builds on sampling the ODE at t{n+Â½}. We can
    evaluate at t{n+Â½} and use an average u at times tn and t{n+1}

                 (uâ¿âºÂ¹ - uâ¿) / Î”t = -a(t{n+Â½}) Â½(uâ¿ + uâ¿âºÂ¹)

    Alternatively, we can use an average for the product a u

               (uâ¿âºÂ¹ - uâ¿) / Î”t = -Â½(a(tn) uâ¿ + a(t{n+1}) uâ¿âºÂ¹)

    The Î¸-tule unifies the three mentioned schemes. One version is to have a
    evaluated at the weighted time point (1 - Î¸) tn + Î¸ t{n+1}

        (uâ¿âºÂ¹ - uâ¿) / Î”t = -a((1 - Î¸) tn + Î¸ t{n+1})((1 - Î¸) uâ¿ + Î¸ uâ¿âºÂ¹)

    Another possibility is to apply a weighted average for the product a u

             (uâ¿âºÂ¹ - uâ¿) / Î”t = -(1 - Î¸) a(tn) uâ¿ + Î¸ a(t{n+1})uâ¿âºÂ¹

    With the finite difference operator notation, the Forward Euler and
    Backward Euler can be summarised as

                              [Dâ‚œâº u = -a u]â¿
                              [Dâ‚œâ» u = -a u]â¿

    The Crank-Nicolson and Î¸ schemes depend on whether we evaluate a at the
    sample point for the ODE or if we use an average.

                            [Dâ‚œ u = -a Å«áµ—]â¿âºÂ¹â¸Â²
                            [Dâ‚œ u = -Ä Å«áµ—]â¿âºÂ¹â¸Â²
                            [DÌ„â‚œ u = -a Å«áµ—Ê¼á¶¿]â¿âºá¶¿
                            [DÌ„â‚œ u = -Ä Å«áµ—Ê¼á¶¿]â¿âºá¶¿

    A further extension of the model ODE is to include a source term b(t):

              u'(t) = a(t) u(t) + b(t)     t âˆˆ (0, T]     u(0) = I

    The time point where we sample the ODE determines where b(t) is evaluated.
    For the Crank-Nicolson scheme and the Î¸-rule, we have a choice of whether
    to evaluate a(t) and b(t) at the correct point, or use an average. The
    chosen strategy becomes particularly clear if we write up the schemes in
    operation notation:

                            [Dâ‚œâº u = -a u + b]â¿
                            [Dâ‚œâ» u = -a u + b]â¿
                            [Dâ‚œ u = -a Å«áµ— + b]â¿âºÂ¹â¸Â²
                            [Dâ‚œ u = -Ä Å« + bÌ„áµ—]â¿âºÂ¹â¸Â²
                            [DÌ„â‚œ u = -a Å«áµ—Ê¼á¶¿ + b]â¿âºá¶¿
                            [DÌ„â‚œ u = -Ä Å« + bÌ„áµ—Ê¼á¶¿]â¿âºá¶¿

    Deriving the Î¸-rule formula when averaging over a and b, we get

        (uâ¿âºÂ¹ - uâ¿) / Î”t = Î¸(-aâ¿âºÂ¹ uâ¿âºÂ¹ + bâ¿âºÂ¹) + (1 - Î¸)(-aâ¿ uâ¿ + bâ¿)

    Solving for uâ¿âºÂ¹,

     uâ¿âºÂ¹ = ((1 - Î”t(1 - Î¸)aâ¿) uâ¿ + Î”t(Î¸ bâ¿âºÂ¹ + (1 - Î¸)bâ¿)) / (1 + Î”t Î¸ aâ¿âºÂ¹)

    Here, we start by verifying a constant solution, where u = C. We choose any
    a(t) and set b(t) = a(t) C and I = C.

    """
    # Definitions
    u_const = 2.15
    I = u_const
    Nt = 4
    dt = 4
    theta = 0.4
    a = lambda t: 2.5 * (1 + t**3)
    b = lambda t: a(t) * u_const

    # Solve
    u, t = solver(I=I, a=a, b=b, T=Nt * dt, dt=dt, theta=theta)

    # Calculate exact solution
    u_exact = lambda t: u_const
    u_e = u_exact(t)

    # Assert that max deviation is below tolerance
    tol = 1E-14
    diff = np.max(np.abs(u_e - u))
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}')


def verification() -> None:
    """
    Verification via manufactured solutions.

    Notes
    ----------
    Following the idea above, we choose any formula as the exact solution,
    insert the formula in the ODE problem and fit the data a(t), b(t), and I to
    make the chosen formula fulfill the equation. This powerful technique for
    generating exact solutions is very useful for verification purposes and
    known as the method of manufactured solutions, often abbreviated MMS.

    One common choice of solution is a linear function in the independent
    variable(s). The rationale behind such a simple variation is that almost
    any relevant numerical solution method for differential equation problems
    is able to reproduce a linear function exactly to machine precision. The
    linear solution also makes some stronger demands to the numerical solution
    and the implementation than the constant solution one.

    We choose a linear solution u(t) = c t + d. From the initial condition, it
    follows that d = I. Inserting this u in the left-hand side of (1), we get

                            c = -a(t) u + b(t)

    Any function u = c t + I is then a correct solution if we choose

                          b(t) = c + a(t)(c t + I)

    Therefore, we must check that uâ¿ = c a(tn)(c tn + I) fulfills the discrete
    equations. For these equations, it is convenient to compute the action of
    the difference operator on a linear function t:

                         [Dâ‚œâº t]â¿ = (tâ¿âºÂ¹ - tâ¿) / Î”t = 1
                         [Dâ‚œâ» t]â¿ = (tâ¿ - tâ¿â»Â¹) / Î”t = 1
                         [Dâ‚œ t]â¿ = (tâ¿âºÂ¹â¸Â² - tâ¿â»Â¹â¸Â²) / Î”t
                                 = ((n + Â½)Î”t - (n - Â½)Î”t) / Î”t = 1

    Clearly, all three difference approximations to the derivative are exact
    for u(t) = t or its mesh function counterpart uâ¿ = tn. The difference
    equation in the Forward Euler scheme

                              [Dâ‚œâº u = -a u + b]â¿

    with aâ¿ = a(tn), bâ¿ = c + a(tn)(c tn + I), and uâ¿ = ctn + I then results in

                  c = -a(tn)(c tn + I) + c + a(tn)(c tn + I) = c

    which is always fulfilled. Similar calculations can be done for the Forward
    Euler and Crank-Nicolson schemes. Therefore, we expect that uâ¿ - uâ‚‘(tn) = 0
    mathematically and |uâ¿ - uâ‚‘(tn)| less than a small number about the machine
    precision.

    """
    # Definitions
    I = 0.1
    T = 4
    dt = 0.1
    Nt = int(T / dt)
    theta = 0.4
    c = -0.5
    u_exact = lambda t: c * t + I
    a = lambda t: t**0.5
    b = lambda t: c + a(t) * u_exact(t)

    # Solve
    u, t = solver(I=I, a=a, b=b, T=Nt * dt, dt=dt, theta=theta)

    # Calculate exact solution
    u_e = u_exact(t)

    # Assert that max deviation is below tolerance
    tol = 1E-14
    diff = np.max(np.abs(u_e - u))
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}')


def convergence() -> None:
    """
    Computing convergence rates.

    Notes
    ----------
    We expect that the error E in the numerical solution is reduced if the mesh
    size Î”t is decreased. More specifically, many numerical methods obey a
    power-law relation between E and Î”t

                                E = C Î”tÊ³

    where C and r are (usually unknown) constant independent of Î”t. The
    parameter r is known as the convergence rate. For example, if the
    convergence rate is 2, halving Î”t reduces the error by a factor of 4.
    Diminishing Î”t then has a greater impact on the error compared with methods
    that have r = 1. For a given value of r, we refer to the method as of r-th
    order.

    There are two alternative ways of estimating C and r based on a set of m
    simulations with corresponding pairs (Î”táµ¢, Eáµ¢), i = 0,...,m-1, and
    Î”táµ¢ < Î”táµ¢â‚‹â‚.
    1) Take the logarithm of E = C Î”tÊ³ and fit a straight line to the data
       points (Î”táµ¢, Eáµ¢).
    2) Consider two consecutive experiments (Î”táµ¢, Eáµ¢) and (Î”táµ¢â‚‹â‚, Eáµ¢â‚‹â‚).
       Dividing the equation Eáµ¢â‚‹â‚ = C Î”tÊ³áµ¢â‚‹â‚ by Eáµ¢ = C Î”tÊ³áµ¢ and solving for r

                      ráµ¢â‚‹â‚ = ln(Eáµ¢â‚‹â‚ / Eáµ¢) / ln(Î”táµ¢â‚‹â‚ / Î”táµ¢)

    for i = 1,...,m-1. Note that we have introduced a subindex i - 1 on r
    because r estimated from a pair of experiments must be expected to change
    with i.

    The disadvantage of method 1 is that it may not be valid for the coarsest
    meshes (largest Î”t values). Fitting a line to all the data points is then
    misleading. Method 2 computes the convergence rates for pairs of
    experiments and allows us to see if the sequence ráµ¢ converges to some
    values as i â†’ m - 2. The final râ‚˜â‚‹â‚‚ can then be taken as the convergence
    rate.

    The strong practical application of computing convergence rates is for
    verification: wrong convergence rates point to errors in the code, and
    correct convergence rates provide strong support for a correct
    implementation. Bugs in the code can easily destroy the expected
    convergence rate.

    The example here used the manufactured solution uâ‚‘(t) = sin(t) exp{-2t} and
    a(t) = tÂ². This implies we must fit b as b(t) = u'(t) + a(t) u(t). We first
    compute with SymPy expressions and then convert the exact solution, a, and
    b to Python functions that we can use in subsequent numerical computing.

    """
    # Created manufactured solution with SymPy
    t = sym.Symbol('t')
    u_e_sym = sym.sin(t) * sym.exp(-2 * t)
    a_sym = t**2
    b_sym = sym.diff(u_e_sym, t) + a_sym * u_e_sym

    # Turn SymPy expressions into Python functions
    u_exact = sym.lambdify([t], u_e_sym, modules='numpy')
    a = sym.lambdify([t], a_sym, modules='numpy')
    b = sym.lambdify([t], b_sym, modules='numpy')

    T = 6
    I = u_exact(0)
    dt_values = [0.1 * 2**(-i) for i in range(8)]
    print(STR_FMT.format('dt_values', f'{dt_values}'))

    th_dict = {0: ('Forward Euler', 'fe', 'r-s'),
               1: ('Backward Euler', 'be', 'g-v'),
               0.5: ('Crank-Nicolson', 'cn', 'b-^')}

    for theta in th_dict:
        print(f'{th_dict[theta][0]}')
        E_values = []
        for dt in dt_values:
            # Solve for mesh function
            u, t = solver(I=I, a=a, b=b, T=T, dt=dt, theta=theta)

            # Compute error
            u_e = u_exact(t)
            e = u_e - u
            E = np.sqrt(dt * np.sum(e**2))
            E_values.append(E)

        # Compute convergence rates
        r = compute_rates(dt_values, E_values)
        print(STR_FMT.format('r', f'{r}'))

        # Test final entry with expected convergence rate
        expected_rate = 2 if theta == 0.5 else 1
        tol = 0.1
        diff = np.abs(expected_rate - r[-1])
        if diff > tol:
            raise AssertionError(f'Tolerance not reached, diff = {diff}')


def systems() -> None:
    """
    Extension to systems of ODEs.

    Notes
    ----------
    Many ODE models involve more than one unknown function and more than one
    equation. Here is an example of two unknown functions u(t) and v(t)

                u' = a u + b v             v' = c u + d v

    for constants a, b, c, d. Applying the Forward Euler method to each
    equation results in a simple updating formula

                       uâ¿âºÂ¹ = uâ¿ + Î”t(a uâ¿ + b vâ¿)
                       vâ¿âºÂ¹ = vâ¿ + Î”t(c uâ¿ + d vâ¿)

    On the other hand, the Crank-Nicolson and Backward Euler schemes result in
    a 2x2 linear system for the new unknowns. The latter scheme becomes

                       uâ¿âºÂ¹ = uâ¿ + Î”t(a uâ¿âºÂ¹ + b vâ¿âºÂ¹)
                       vâ¿âºÂ¹ = vâ¿ + Î”t(c uâ¿âºÂ¹ + d vâ¿âºÂ¹)

    Collecting uâ¿âºÂ¹ as well as vâ¿âºÂ¹ on the left-hand side results in

                        (1 - Î”t a)uâ¿âºÂ¹ + b vâ¿âºÂ¹ = uâ¿
                        c uâ¿âºÂ¹ + (1 - Î”t d)vâ¿âºÂ¹ = vâ¿

    which is a system of two coupled, linear, algebraic equations in two
    unknowns. These equations can be solved algebraically resulting in explicit
    forms for uâ¿âºÂ¹ and vâ¿âºÂ¹ that can be directly implemented. For a system of
    ODEs with many equations and unknowns, one will express the coupled
    equations at each time level in matrix form and call software for numerical
    solution of linear systems of equations.

    Deriving the Î¸-rule formula we get

           (uâ¿âºÂ¹ - uâ¿) / Î”t = Î¸(a uâ¿âºÂ¹ + b vâ¿âºÂ¹) + (1 - Î¸)(a uâ¿ + b vâ¿)
           (vâ¿âºÂ¹ - vâ¿) / Î”t = Î¸(c uâ¿âºÂ¹ + d vâ¿âºÂ¹) + (1 - Î¸)(c uâ¿ + v uâ¿)

    Collecting uâ¿âºÂ¹ as well as vâ¿âºÂ¹ on the left-hand side results in

        (Î”t a Î¸ - 1) uâ¿âºÂ¹ + b Î”t Î¸ vâ¿âºÂ¹ = (a Î”t(Î¸ - 1) - 1)uâ¿ + b Î”t(Î¸ - 1)vâ¿
        c Î”t Î¸ uâ¿âºÂ¹ + (d Î”t Î¸ - 1) vâ¿âºÂ¹ =  c Î”t(Î¸ - 1)uâ¿ + (d Î”t(Î¸ - 1) - 1)vâ¿

    Which can be solved for uâ¿âºÂ¹ and vâ¿âºÂ¹ at each time step, n.

    """
    # Definitions
    I = 1
    T = 4
    dt = 0.1
    Nt = int(T / dt)
    theta = 0.5
    a = -1
    b = -2
    c = 3
    d = -1
    u_exact = lambda t: (1/3) * np.exp(-t) * \
        (3 * np.cos(np.sqrt(6) * t) - np.sqrt(6) * np.sin(np.sqrt(6) * t))
    v_exact = lambda t: (1/2) * np.exp(-t) * \
        (2 * np.cos(np.sqrt(6) * t) + np.sqrt(6) * np.sin(np.sqrt(6) * t))

    # Define mesh functions and points
    u = np.zeros(Nt + 1)
    v = np.zeros(Nt + 1)
    t = np.linspace(0, Nt * dt, Nt + 1)
    u[0] = I
    v[0] = I

    # Solve using difference equations
    # (Î”t a Î¸ - 1) uâ¿âºÂ¹ + b Î”t Î¸ vâ¿âºÂ¹ = (a Î”t(Î¸ - 1) - 1)uâ¿ + b Î”t(Î¸ - 1)vâ¿
    # c Î”t Î¸ uâ¿âºÂ¹ + (d Î”t Î¸ - 1) vâ¿âºÂ¹ =  c Î”t(Î¸ - 1)uâ¿ + (d Î”t(Î¸ - 1) - 1)vâ¿
    A = [[dt * a * theta - 1, b * dt * theta],
         [c * dt * theta, d * dt * theta - 1]]
    for n in range(Nt):
        B = [(a * dt * (theta - 1) - 1) * u[n] + b * dt * (theta - 1) * v[n],
             c * dt * (theta - 1) * u[n] + (d * dt * (theta - 1) - 1) * v[n]]
        u[n + 1], v[n + 1] = scipy.linalg.solve(A, B)
    print(STR_FMT.format('u', u))
    print(STR_FMT.format('v', v))

    # Compute error
    u_e = u_exact(t)
    e = u_e - u

    # Assert that max deviation is below tolerance
    tol = 1E-2
    diff = np.max(np.abs(u_e - u))
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}')


def generic_FO_ODE() -> None:
    """
    Generic first-order ODEs.

    Notes
    ----------
    We now turn the attention to general, nonlinear ODEs and systems of such
    ODEs. Our focus is on numerical methods that can be readily reused for
    discretisation of PDEs, and diffusion PDEs in particular.

    ODEs are commonly written in the generic form

                           u' = f(u, t),        u(0) = I

    where f(u, t) is some prescribed function. The unknown u may either be a
    scalar function of time t, or a vector values function of t in the case of
    a system of ODEs with m unknown components

                    u(t) = (uâ½â°â¾(t), uâ½Â¹â¾(t), ..., uâ½áµâ»Â¹â¾(t))

    In that case, the right-hand side is a vector-valued function with m
    components

            f(u, t) = (fâ½â°â¾(uâ½â°â¾(t), uâ½Â¹â¾(t), ..., uâ½áµâ»Â¹â¾(t)),
                       fâ½Â¹â¾(uâ½â°â¾(t), uâ½Â¹â¾(t), ..., uâ½áµâ»Â¹â¾(t)),
                       ...
                       fâ½áµâ»Â¹â¾(uâ½â°â¾(t), uâ½Â¹â¾(t), ..., uâ½áµâ»Â¹â¾(t)))

    Actually, any system of ODEs can be written in the form (2), but
    higher-order ODEs then need auxiliary unknown functions to enable
    conversion to a first-order system.

    The Î¸-rule scheme applied to u' = f(u, t) becomes

            (uâ¿âºÂ¹ - uâ¿) / Î”t = Î¸ f(uâ¿âºÂ¹, t{n+1}) + (1 - Î¸) f(uâ¿, tn})

    Bring the unknown uâ¿âºÂ¹ to the left-hand side and the known terms on the
    right-hand side gives

            uâ¿âºÂ¹ - Î”t Î¸ f(uâ¿âºÂ¹, t{n+1}) = uâ¿ + Î”t(1 - Î¸) f(uâ¿, tn)

    For a general f (not linear in u), this equation is nonlinear in the
    unknown uâ¿âºÂ¹ unless Î¸ = 0. For a scalar ODE (m = 1), we have to solve a
    single nonlinear algebraic equation for uâ¿âºÂ¹, while for a system of ODEs,
    we get a system of coupled, nonlinear algebraic equations. Newton's method
    is a popular solution approach in both cases. Note that with the Forward
    Euler scheme (Î¸ = 0), we do not have to deal with nonlinear equations,
    because in that case we have an explicit updating formula for uâ¿âºÂ¹. This is
    known as an explicit scheme. With Î¸ â‰  0, we have to solve (systems of)
    algebraic equations, and the scheme is said to be implicit.

    Here we demonstrate using the ODE u'(t) = u(t)Â² - 2, u(0) = 1. Then,

            uâ¿âºÂ¹ - Î”t Î¸ ((uâ¿âºÂ¹)Â² - 2) = uâ¿ + Î”t(1 - Î¸) ((uâ¿)Â² - 2)
          y = Î”t Î¸ (uâ¿âºÂ¹)Â² - uâ¿âºÂ¹ + Î”t (uâ¿)Â² (1 - Î¸) + uâ¿ - 2 Î”t = 0

    The derivative of this can be used to solve for uâ¿âºÂ¹ using Newton's method

                        dy/duâ¿âºÂ¹ = 2 Î”t Î¸ uâ¿âºÂ¹ - 1

    """
    # Definitions
    I = 1
    T = 4
    dt = 0.1
    Nt = int(T / dt)
    theta = 0.5
    sqrt2 = np.sqrt(2)
    f_ut = lambda u, t: u(t)**2 - 2
    u_exact = lambda t: (2 + sqrt2 + (-2 + sqrt2) * np.exp(2 * sqrt2 * t)) / \
        (1 + sqrt2 + (-1 + sqrt2) * np.exp(2 * sqrt2 * t))

    # Define mesh functions and points
    u = np.zeros(Nt + 1)
    t = np.linspace(0, Nt * dt, Nt + 1)
    u[0] = I

    # Solve using difference equation
    # uâ¿âºÂ¹ - Î”t Î¸ f(uâ¿âºÂ¹, t{n+1}) = uâ¿ + Î”t(1 - Î¸) f(uâ¿, tn)
    # y = Î”t Î¸ (uâ¿âºÂ¹)Â² - uâ¿âºÂ¹ + Î”t (uâ¿)Â² (1 - Î¸) + uâ¿ - 2 Î”t = 0
    y = lambda u1, u0: dt * theta * u1**2 - u1 + dt * u0**2 * (1 - theta) + \
        u0 - 2 * dt
    dy = lambda u1: 2 * dt * theta * u1 - 1
    for n in range(Nt):
        u[n + 1] = scipy.optimize.fsolve(
            partial(y, u0=u[n]), u[n], fprime=dy
        )[0]
    print(STR_FMT.format('u', u))

    # Compute error
    u_e = u_exact(t)
    e = u_e - u

    # Assert that max deviation is below tolerance
    tol = 1E-2
    diff = np.max(np.abs(u_e - u))
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}')


def bdf2() -> List[List[float]]:
    """
    An implicit 2-step backward scheme.

    Notes
    ----------
    The implicit backward method with 2 steps applies a three-level backward
    difference as approximation to u'(t).

                u'(t{n+1}) â‰ˆ (3 uâ¿âºÂ¹ - 4 uâ¿ + uâ¿â»Â¹) / (2 Î”t)

    which is an approximation of order Î”tÂ² to the first derivative. The
    resulting scheme for u' = f(u, t) reads

            uâ¿âºÂ¹ = (4/3) uâ¿ - (1/3) uâ¿â»Â¹ + (2/3) Î”t f(uâ¿âºÂ¹, t{n+1})

    Higher-order versions of this scheme can be constructed by including more
    time levels. These schemes are known as Backward Differentiation Formulas
    (BDF), and this particular version is referred to as BDF2 and has second
    order convergence.

    Also note that this scheme is implicit and requires solution of nonlinear
    equations when f is nonlinear in u. The standard 1st-order Backward Euler
    method or Crank-Nicolson scheme can be used for the first step.

    Returns
    ----------
    solutions: Numerical solutions for 8 Î”t values.

    """
    # Definitions
    # Solving u'(t) = -a * u(t)
    I = 1
    T = 4
    a = 3
    u_exact = lambda t: np.exp(-a * t)

    dt_values = [0.1 * 2**(-i) for i in range(8)]
    print(STR_FMT.format('dt_values', f'{dt_values}'))

    solutions = []
    E_values = []
    for dt in dt_values:
        # Define mesh functions and points
        Nt = int(T / dt)
        u = np.zeros(Nt + 1)
        t = np.linspace(0, Nt * dt, Nt + 1)
        u[0] = I

        # Solve for mesh function
        first_step = 'BackwardEuler'
        if first_step == 'CrankNicolson':
            # Crank-Nicolson 1. step
            u[1] = (1 - 0.5 * a * dt) / (1 + 0.5 * dt * a) * u[0]
        elif first_step == 'BackwardEuler':
            # Backward Euler 1. step
            u[1] = 1 / (1 + dt * a) * u[0]
        for n in range(1, Nt):
            u[n + 1] = (4 * u[n] - u[n - 1]) / (3 + 2 * dt * a)

        # Compute error
        u_e = u_exact(t)
        e = u_e - u
        E = np.sqrt(dt * np.sum(e**2))
        E_values.append(E)

        solutions.append(u)

    # Compute convergence rates
    r = compute_rates(dt_values, E_values)
    print(STR_FMT.format('r', f'{r}'))

    # Test final entry with expected convergence rate
    expected_rate = 2
    tol = 0.1
    diff = np.abs(expected_rate - r[-1])
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}!={r[-1]}')

    return solutions


def leapfrog() -> List[List[float]]:
    """
    Leapfrog schemes.

    Notes
    ----------
    The ordinary Leapfrog scheme
        The derivative of u at some point tn can be approximated by a central
        difference over two time steps,

                      u'(tn) â‰ˆ (uâ¿âºÂ¹ - uâ¿â»Â¹) / (2 Î”t) = [Dâ‚‚â‚œ u]â¿

        which is an approximation of second order in Î”t. The scheme can then be
        written as

                                 [Dâ‚‚â‚œ u = f(u, t)]â¿

        in the operator notation. Solving for uâ¿âºÂ¹ gives

                             uâ¿âºÂ¹ = uâ¿â»Â¹ + 2 Î”t f(uâ¿, tn)

        Observe that this is an explicit scheme, and that a nonlinear f (in u)
        is trivial to handle since it only involves the known uâ¿ values. Some
        other scheme must be used as a starter to compute uÂ¹, preferable the
        Forward Euler scheme since it is also explicit.

    The filtered Leapfrog scheme
        Unfortunately, the ordinary Leapfrog scheme may develop growing
        oscillations with time. A remedy for such undesired oscillation is to
        introduce a filtering technique.

        First, a standard Leapfrog step is taken, and then the previous uâ¿
        value is adjusted according to

                           uâ¿ â† uâ¿ + Î³(uâ¿â»Â¹ - 2 uâ¿ + uâ¿âºÂ¹)

        The Î³-terms will effectively damp oscillations in the solution,
        especially those with short wavelength (like point-to-point
        oscillations). A common choice of Î³ is 0.6 (a values used in the famous
        NCAR Climate Model).

    Returns
    ----------
    solutions: Numerical solutions for 8 Î”t values.

    """
    # Definitions
    # Solving u'(t) = -a * u(t)
    I = 1
    T = 4
    a = 3
    gamma = 0.6
    u_exact = lambda t: np.exp(-a * t)

    dt_values = [0.1 * 2**(-i) for i in range(8)]
    print(STR_FMT.format('dt_values', f'{dt_values}'))

    solutions = []
    E_values = []
    E_filt_values = []
    for dt in dt_values:
        # Define mesh functions and points
        Nt = int(T / dt)
        u = np.zeros(Nt + 1)
        u_filt = np.zeros(Nt + 1)
        t = np.linspace(0, Nt * dt, Nt + 1)
        u[0] = I
        u_filt[0] = I

        # Solve for mesh function
        first_step = 'ForwardEuler'
        if first_step == 'CrankNicolson':
            # Crank-Nicolson 1. step
            u[1] = (1 - 0.5 * a * dt) / (1 + 0.5 * dt * a) * u[0]
        elif first_step == 'ForwardEuler':
            # Forward Euler 1. step
            u[1] = (1 - dt * a) * u[0]
        u_filt[1] = u[1]

        for n in range(1, Nt):
            # Ordinary
            u[n + 1] = u[n - 1] - 2 * dt * a * u[n]

            # Filtered
            u_filt[n + 1] = u_filt[n - 1] - 2 * dt * a * u_filt[n]
            u_filt[n] = u_filt[n] + gamma * (
                u_filt[n - 1] - 2 * u_filt[n] + u_filt[n + 1]
            )

        # Compute error
        u_e = u_exact(t)
        e = u_e - u
        E = np.sqrt(dt * np.sum(e**2))
        E_values.append(E)

        e_filt = u_e - u_filt
        E_filt = np.sqrt(dt * np.sum(e_filt**2))
        E_filt_values.append(E_filt)

        solutions.append(u_filt)


    # Compute convergence rates
    r = compute_rates(dt_values, E_values)
    print(STR_FMT.format('E_values', f'{E_values}'))
    print(STR_FMT.format('r', f'{r}'))
    r_filt = compute_rates(dt_values, E_filt_values)
    print(STR_FMT.format('E_filt_values', f'{E_filt_values}'))
    print(STR_FMT.format('r_filt', f'{r_filt}'))

    # Test final entry with expected convergence rate
    expected_rate = 2
    tol = 0.1
    diff = np.abs(expected_rate - r[-1])
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}!={r[-1]}')

    expected_rate = 1
    diff_filt = np.abs(expected_rate - r_filt[-1])
    if diff_filt > tol:
        raise AssertionError(
            f'Tolerance not reached, diff_filt = {diff_filt}!={r_filt[-1]}'
        )

    return solutions


def rk2() -> List[List[float]]:
    """
    The 2nd-order Runge-Kutta method.

    Notes
    ----------
    The two-step scheme

                            u* = uâ¿ + Î”t f(uâ¿, tn)
                    uâ¿âºÂ¹ = uâ¿ + Î”t Â½ (f(uâ¿, tn) + f(u*, t{n+1}))

    essentially applied a Crank-Nicolson method to the ODE, but replaces the
    term f(uâ¿âºÂ¹, t{n+1}) by a prediction f(u*, t{n+1}) based on the Forward
    Euler step. This scheme is known as Huen's method, but it also a 2nd-order
    Runge-Kutta method. The scheme is explicit, and the error is expected to
    behave as Î”tÂ².

    Returns
    ----------
    solutions: Numerical solutions for 8 Î”t values.

    """
    # Definitions
    # Solving u'(t) = -a * u(t)
    I = 1
    T = 4
    a = 3
    u_exact = lambda t: np.exp(-a * t)

    dt_values = [0.1 * 2**(-i) for i in range(8)]
    print(STR_FMT.format('dt_values', f'{dt_values}'))

    solutions = []
    E_values = []
    for dt in dt_values:
        # Define mesh functions and points
        Nt = int(T / dt)
        u = np.zeros(Nt + 1)
        t = np.linspace(0, Nt * dt, Nt + 1)
        u[0] = I

        # Solve for mesh function
        for n in range(Nt):
            u_star = u[n] - dt * a * u[n]
            u[n + 1] = u[n] - (1/2) * dt * a * (u[n] + u_star)

        # Compute error
        u_e = u_exact(t)
        e = u_e - u
        E = np.sqrt(dt * np.sum(e**2))
        E_values.append(E)

        solutions.append(u)

    # Compute convergence rates
    r = compute_rates(dt_values, E_values)
    print(STR_FMT.format('r', f'{r}'))

    # Test final entry with expected convergence rate
    expected_rate = 2
    tol = 0.1
    diff = np.abs(expected_rate - r[-1])
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}!={r[-1]}')

    return solutions


def taylor_series() -> List[List[float]]:
    """
    2nd-order Taylor-series method.

    Notes
    ----------
    One way to compute uâ¿âºÂ¹ given uâ¿ is to use a Taylor polynomial. We may
    write up a polynomial of 2nd degree

                    uâ¿âºÂ¹ = uâ¿ + u'(tn) Î”t + Â½ u''(tn) Î”tÂ²

    From the equation u' = f(u, t), it follows that the derivatives of u can be
    expressed in terms of f and its derivatives

                            u'(tn) = f(uâ¿, tn)
                    u''(tn) = âˆ‚f/âˆ‚u(uâ¿, tn) u'(tn) + âˆ‚f/âˆ‚t
                            = f(uâ¿, tn) âˆ‚f/du(uâ¿, tn) + âˆ‚f/dt

    resulting in the scheme

        uâ¿âºÂ¹ = uâ¿ + f(uâ¿, tn) Î”t + Â½(f(uâ¿, tn) âˆ‚f/du(uâ¿, tn) + âˆ‚f/dt) Î”tÂ²

    More terms in the series could be included in the Taylor polynomial to
    obtain methods of higher order than 2.

    Returns
    ----------
    solutions: Numerical solutions for 8 Î”t values.

    """
    # Definitions
    # Solving f(t) = u'(t) = -a * u(t)
    I = 1
    T = 4
    a = 3
    f = lambda u, n: -a * u[n]
    dfdu = lambda u, n: -a
    dfdt = lambda u, n: 0
    u_exact = lambda t: np.exp(-a * t)

    dt_values = [0.1 * 2**(-i) for i in range(8)]
    print(STR_FMT.format('dt_values', f'{dt_values}'))

    solutions = []
    E_values = []
    for dt in dt_values:
        # Define mesh functions and points
        Nt = int(T / dt)
        u = np.zeros(Nt + 1)
        t = np.linspace(0, Nt * dt, Nt + 1)
        u[0] = I

        # Solve for mesh function
        for n in range(Nt):
            u[n + 1] = u[n] + f(u, n) * dt + (1/2) * (
                f(u, n) * dfdu(u, t) + dfdt(u, t)
            ) * dt**2

        # Compute error
        u_e = u_exact(t)
        e = u_e - u
        E = np.sqrt(dt * np.sum(e**2))
        E_values.append(E)

        solutions.append(u)

    # Compute convergence rates
    r = compute_rates(dt_values, E_values)
    print(STR_FMT.format('r', f'{r}'))

    # Test final entry with expected convergence rate
    expected_rate = 2
    tol = 0.1
    diff = np.abs(expected_rate - r[-1])
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}!={r[-1]}')

    return solutions


def adams_bashforth() -> List[List[float]]:
    """
    The 2nd- and 3rd-order Adams-Bashforth schemes.

    Notes
    ----------
    The following method is known as the 2nd-order Adams-Bashforth scheme

                uâ¿âºÂ¹ = uâ¿ + Â½ Î”t (3 f(uâ¿, tn) - f(uâ¿â»Â¹, t{n-1}))

    This scheme is explicit and requires another one-step scheme to compute uÂ¹
    (the Forward Euler of Huen's method, for instance). As the name implies,
    the error behaves as Î”tÂ².

    Another explicit scheme, involving four time levels, is the 3rd-order
    Adams-Bashforth scheme

  uâ¿âºÂ¹ = uâ¿ + (1/12) Î”t (23 f(uâ¿, tn) - 16 f(uâ¿â»Â¹, t{n-1}) + 5 f(uâ¿â»Â², t{n-2}))

    The numerical error is of order Î”tÂ³, and the scheme needs some method for
    computing uÂ¹ and uÂ².

    More general, higher-order Adams-Bashforth schemes (also called explicit
    Adams methods) compute uâ¿âºÂ¹ as a linear combination of f at k+1 previous
    time steps:

                      uâ¿âºÂ¹ = uâ¿ + âˆ‘â±¼â‚Œâ‚€áµ Î²â±¼ f(uâ¿â»Ê², t{n-j})

    where Î²â±¼ are known coefficients.

    Returns
    ----------
    solutions: Numerical solutions for 8 Î”t values.

    """
    # Definitions
    # Solving f(t) = u'(t) = -a * u(t)
    I = 1
    T = 4
    a = 3
    f = lambda u, n: -a * u[n]
    u_exact = lambda t: np.exp(-a * t)

    dt_values = [0.1 * 2**(-i) for i in range(8)]
    print(STR_FMT.format('dt_values', f'{dt_values}'))

    solutions = []
    E_2_values = []
    E_3_values = []
    for dt in dt_values:
        # Define mesh functions and points
        Nt = int(T / dt)
        u_2 = np.zeros(Nt + 1)
        u_3 = np.zeros(Nt + 1)
        t = np.linspace(0, Nt * dt, Nt + 1)
        u_2[0] = I

        # Crank-Nicolson 1. and 2. step
        u_2[1] = (1 - (1/2) * a * dt) / (1 + (1/2) * dt * a) * u_2[0]
        u_2[2] = (1 - (1/2) * a * dt) / (1 + (1/2) * dt * a) * u_2[1]
        u_3[:3] = deepcopy(u_2[:3])

        # Solve for mesh function
        for n in range(2, Nt):
            u_2[n + 1] = u_2[n] + (1/2) * dt * (3 * f(u_2, n) - f(u_2, n - 1))
            u_3[n + 1] = u_3[n] + (1/12) * dt * (
                23 * f(u_3, n) - 16 * f(u_3, n - 1) + 5 * f(u_3, n - 2)
            )

        # Compute error
        u_e = u_exact(t)
        e = u_e - u_2
        E = np.sqrt(dt * np.sum(e**2))
        E_2_values.append(E)

        # Compute error
        e = u_e - u_3
        E = np.sqrt(dt * np.sum(e**2))
        E_3_values.append(E)

        solutions.append(u_3)

    # Compute convergence rates
    r_2 = compute_rates(dt_values, E_2_values)
    print(STR_FMT.format('r_2', f'{r_2}'))

    r_3 = compute_rates(dt_values, E_3_values)
    print(STR_FMT.format('r_3', f'{r_3}'))

    # Test final entry with expected convergence rate
    expected_rate = 2
    tol = 0.1
    diff = np.abs(expected_rate - r_2[-1])
    if diff > tol:
        raise AssertionError(
            f'Tolerance not reached, diff = {diff}!={r_2[-1]}'
        )

    # Test final entry with expected convergence rate
    expected_rate = 3
    tol = 0.1
    diff = np.abs(expected_rate - r_3[-1])
    if diff > tol:
        raise AssertionError(
            f'Tolerance not reached, diff = {diff}!={r_3[-1]}'
        )

    return solutions


def rk4() -> List[List[float]]:
    """
    The 4nd-order Runge-Kutta method.

    Notes
    ----------
    Perhaps the most widely used method to solve ODEs is the 4th-order
    Runge-Kutta method, often called RK4. Its derivation is a nice illustration
    of common numerical approximation strategies.

    The starting point is to integrate the ODE u' = f(u, t) from tn to t{n+1}

                u(t{n+1}) - u(tn) = âˆ«â‚œâ‚™áµ—â¿âºÂ¹ f(u(t), t) dt

    We want to compute u(t{n+1}) and regard u(tn) as known. The task is to find
    good approximations for the integral, since the integrand involves the
    unknown u between tn and t{n+1}.

    The integrand can be approximated by the famous Simpson's rule

            âˆ«â‚œâ‚™áµ—â¿âºÂ¹ f(u(t), t) dt â‰ˆ Î”t / 6 (fâ¿ + 4 fâ¿âºÂ¹â¸Â² + fâ¿âºÂ¹)

    The problem is that we do not know fâ¿âºÂ¹â¸Â² and fâ¿âºÂ¹ as we know only uâ¿ and
    hence fâ¿. The idea is to use various approximations for fâ¿âºÂ¹â¸Â² and fâ¿âºÂ¹
    based on well-known schemes for the ODE in the intervals [tn, t{n+Â½}] and
    [tn, t{n+1}]. we split the integral approximation into four terms

         âˆ«â‚œâ‚™áµ—â¿âºÂ¹ f(u(t), t) dt â‰ˆ Î”t / 6 (fâ¿ + 2 ð’‡â¿âºÂ¹â¸Â² + 2 ð•—â¿âºÂ¹â¸Â² + á¸Ÿâ¿âºÂ¹)

    where ð’‡â¿âºÂ¹â¸Â², ð•—â¿âºÂ¹â¸Â², and á¸Ÿâ¿âºÂ¹ are approximations to fâ¿âºÂ¹â¸Â² and fâ¿âºÂ¹
    respectively, that can be based on already computed quantities. For ð’‡â¿âºÂ¹â¸Â²
    we can apply an approximation to uâ¿âºÂ¹â¸Â² using the Forward Euler method with
    step Â½ Î”t

                        ð’‡â¿âºÂ¹â¸Â² = f(uâ¿ + Â½ Î”t fâ¿, t{n+Â½})

    Since this gives us a prediction of fâ¿âºÂ¹â¸Â², we can for ð•—â¿âºÂ¹â¸Â² try a
    Backward Euler method to approximate uâ¿âºÂ¹â¸Â²

                     ð•—â¿âºÂ¹â¸Â² = f(uâ¿ + Â½ Î”t ð’‡â¿âºÂ¹â¸Â², t{n+Â½})

    With ð•—â¿âºÂ¹â¸Â² as a hopefully good approximation to fâ¿âºÂ¹â¸Â², we can, for the
    final term á¸Ÿâ¿âºÂ¹, use a Crank-Nicolson method on [tn, t{n+1}] to approximate
    uâ¿âºÂ¹

                     á¸Ÿâ¿âºÂ¹â¸Â² = f(uâ¿ + Î”t ð•—â¿âºÂ¹â¸Â², t{n+1})

    We have now used the Forward and Backward Euler methods as well as the
    Crank-Nicolson method in the context of Simpson's rule. The hope is that
    the combination of these methods yields an overall time-stepping scheme
    from tn to t{n+1} that is much more accurate that the ð“ž(Î”t) and ð“ž(Î”tÂ²) of
    the individual steps. This in indeed true: the overall accuracy is ð“ž(Î”tâ´)!

    To summarise, the 4th-order Runge-Kutta method becomes

               uâ¿âºÂ¹ = uâ¿ + Î”t / 6 (fâ¿ + 2 ð’‡â¿âºÂ¹â¸Â² + 2 ð•—â¿âºÂ¹â¸Â² + á¸Ÿâ¿âºÂ¹)

    where the quantities on the right-hand side are computed as defined above.
    Note that the scheme is fully explicit, so there is never any need to solve
    linear or nonlinear algebraic equations. However, the stability is
    conditional and depends on f. There is a whole range of implicit
    Runge-Kutta methods that are unconditionally stable, but require solution
    of algebraic equations involving f at each time step.

    Returns
    ----------
    solutions: Numerical solutions for 8 Î”t values.

    """
    # Definitions
    # Solving f(t) = u'(t) = -a * u(t)
    I = 1
    T = 4
    a = 3
    f = lambda u, n: -a * u[n]
    u_exact = lambda t: np.exp(-a * t)

    dt_values = [0.1 * 2**(-i) for i in range(8)]
    print(STR_FMT.format('dt_values', f'{dt_values}'))

    solutions = []
    E_values = []
    for dt in dt_values:
        # Define mesh functions and points
        Nt = int(T / dt)
        u = np.zeros(Nt + 1)
        t = np.linspace(0, Nt * dt, Nt + 1)
        u[0] = I

        # Solve for mesh function
        for n in range(Nt):
            f_fe = -a * (u[n] + (1/2) * dt * f(u, n))
            f_be = -a * (u[n] + (1/2) * dt * f_fe)
            f_cn = -a * (u[n] + dt * f_be)
            u[n + 1] = u[n] + (1/6) * dt * (
                f(u, n) + 2 * f_fe + 2 * f_be + f_cn
            )

        # Compute error
        u_e = u_exact(t)
        e = u_e - u
        E = np.sqrt(dt * np.sum(e**2))
        E_values.append(E)

        solutions.append(u)

    # Compute convergence rates
    r = compute_rates(dt_values, E_values)
    print(STR_FMT.format('r', f'{r}'))

    # Test final entry with expected convergence rate
    expected_rate = 4
    tol = 0.1
    diff = np.abs(expected_rate - r[-1])
    if diff > tol:
        raise AssertionError(f'Tolerance not reached, diff = {diff}!={r[-1]}')

    return solutions


def compare_schemes() -> None:
    """
    Compare schemes discussed in this chapter.
    """
    schemes = {'Filtered Leapfrog': leapfrog,
               '2nd-order Taylor-series': taylor_series,
               '3rd-order Adams-Bashforth': adams_bashforth,
               '2nd-order Runge-Kutta': rk2,
               '4th-order Runge-Kutta': rk4}

    # Definitions
    # Solving f(t) = u'(t) = -a * u(t)
    I = 1
    T = 4
    a = 3
    f = lambda u, n: -a * u[n]
    u_exact = lambda t: np.exp(-a * t)

    dt_values = [0.1 * 2**(-i) for i in range(8)]

    fig = plt.figure(figsize=(24, 10))
    fig.suptitle(
        r'$\frac{du(t)}{dt}=-a\cdot u(t)\:{\rm where}\:a<0$', y=0.95
    )

    axs = []
    gs = gridspec.GridSpec(2, 4)
    gs.update(hspace=0.3, wspace=0.3)
    for sc_idx, scheme in enumerate(schemes):
        # Get solution
        solutions = schemes[scheme]()

        for dt_idx, dt in enumerate(dt_values):
            Nt = int(T / dt)
            t = np.linspace(0, Nt * dt, Nt + 1)

            if sc_idx == 0:
                gssub = gs[dt_idx].subgridspec(
                    2, 1, height_ratios=(1, 1), hspace=0
                )
                ax1 = fig.add_subplot(gssub[0])
                ax2 = fig.add_subplot(gssub[1])
                axs.append([ax1, ax2])

                ax1.set_title('$\Delta t={:g}$'.format(dt))
                ax1.set_ylim(0, 1)
                ax1.set_xlim(0, T)
                ax1.set_ylabel('u')
                ax1.set_xticks([])
                ax1.set_yticks(ax1.get_yticks()[1:])

                ax2.grid(c='k', ls='--', alpha=0.3)
                ax2.set_yscale('log')
                ax2.set_xlim(0, T)
                ax2.set_xlabel('t')
                ax2.set_ylabel('log err')

                # Calculate exact solution
                t_e = np.linspace(0, T, 1001)
                u_e = u_exact(t_e)

                # Plot with black line
                ax1.plot(t_e, u_e, 'k-', label='exact')

            ax1, ax2 = axs[dt_idx]

            # Plot
            p = ax1.plot(t, solutions[dt_idx], ls='-', label=scheme)
            ax1.legend()

            # Calculate exact solution
            u_e = u_exact(t)
            err = np.abs(u_e - solutions[dt_idx])
            ax2.plot(t, err, ls='-', color=p[0].get_color())

    # Save figure
    fig.savefig(IMGDIR + f'comparison.png', bbox_inches='tight')


def main() -> None:
    """Main program, used when run as a script."""
    import argparse
    parser = argparse.ArgumentParser(
        description=
        'Finite Difference Computing with Exponential Decay Models - Chapter 3'
    )
    parser.add_argument('functions', nargs='*', help=f'Choose from {__all__}')
    args = parser.parse_args()

    functions = args.functions if args.functions else __all__
    for f in functions:
        if f not in __all__:
            raise ValueError(f'Invalid function "{f}" (choose from {__all__})')
        print('------', f'\nRunning "{f}"')
        globals()[f]()
        print('------')


main.__doc__ = __doc__


if __name__ == "__main__":
    main()
